model_parameters:
    name : 'NTM'
    encoder :
        name : 'PTFCEncoder'
        input_dim : 2000
        hidden_dim : 500
        latent_dim : 50
        architecture : 'NTM'
    decoder :
        name : 'PTFCDecoder'
        latent_dim : 50
        output_dim : 2000
        batch_size : 100
        architecture : 'NTM'


experiment_parameters:
  name : 'PTVAEXperiment'
  seed : 50
  dataset : 20news
  data_path : 'resources/datasets/20news/1'
  sparse: False
  labels: True
  save_path : 'results/'
  batch_size : 100
  learning_rate : 5e-5
  use_cuda : True


trainer_parameters:
  max_epochs : 1000
  patience : 0 # used for early stopping , ignored if set to 0 - also depends on test_interval
  log_interval : 10 # log every 10th batch
  test_interval : 1  # evaluate on test(valid) every epoch
  track:
    loss_components: False # track and log all components of the loss
    perplexity: False


